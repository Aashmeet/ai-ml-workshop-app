{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SageMaker/DeepAR demo on household electricity consumption dataset\n",
    "\n",
    "This notebook complements the following two notebooks:\n",
    "* [DeepAR introduction notebook](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/deepar_synthetic/deepar_synthetic.ipynb). \n",
    "* [Individual household electric power consumption dataset](https://github.com/amirrezaeian/Individual-household-electric-power-consumption-Data-Set-/blob/master/data_e_power.ipynb).\n",
    "\n",
    "The household electric power consumption dataset is available at:<br>\n",
    "http://archive.ics.uci.edu/ml/datasets/Individual+household+electric+power+consumption\n",
    "\n",
    "In summary, the dataset consists of the following attributes:\n",
    "* `date`: date (dd/mm/yyyy)\n",
    "* `time`: time (hh:mm:ss)\n",
    "* `global_active_power`: household global minute-averaged active power (in Kilowatt) \n",
    "* `global_reactive_power`: household global minute-averaged reactive power (in Kilowatt)\n",
    "* `voltage`: minute-averaged voltage (in Volt)\n",
    "* `global_intensity`: household global minute-averaged current intensity (in Ampere) \n",
    "* `sub_metering_1`: energy sub-metering No.1 (in Watt-per-hour of active energy). It corresponds to the kitchen, containing mainly a dishwasher, an oven and a microwave (hot plates are not electric but gas powered). \n",
    "* `sub_metering_2`: energy sub-metering No.2 (in Watt-per-hour of active energy). It corresponds to the laundry room, containing a washing-machine, a tumble-drier, a refrigerator and a light. \n",
    "* `sub_metering_3`: energy sub-metering No.3 (in Watt-per-hour of active energy). It corresponds to an electric water-heater and an air-conditioner. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In particular, we will see how to:\n",
    "* Prepare the dataset\n",
    "* Use the SageMaker Python SDK to train a DeepAR model and deploy it\n",
    "* Make requests to the deployed model to obtain forecasts interactively\n",
    "* Illustrate advanced features of DeepAR: missing values, additional time features, non-regular frequencies and category information\n",
    "\n",
    "For more information about DeepAR, see the [documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/deepar.html) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import zipfile\n",
    "import random\n",
    "import datetime\n",
    "from urllib.request import urlretrieve\n",
    "from dateutil.parser import parse\n",
    "from random import shuffle\n",
    "\n",
    "import boto3\n",
    "import s3fs\n",
    "import sagemaker\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import randint\n",
    "import seaborn as sns # used for plot interactive graph. \n",
    "\n",
    "from __future__ import print_function\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import IntSlider, FloatSlider, Checkbox\n",
    "\n",
    "from sklearn.model_selection import train_test_split # to split the data into two parts\n",
    "from sklearn.cross_validation import KFold # use for cross validation\n",
    "from sklearn.preprocessing import StandardScaler # for normalization\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline # pipeline making\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn import metrics # for the check the error and accuracy of the model\n",
    "from sklearn.metrics import mean_squared_error,r2_score\n",
    "\n",
    "## for Deep-learing:\n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "# from keras.utils import to_categorical\n",
    "from keras.optimizers import SGD \n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import np_utils\n",
    "import itertools\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers import Dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting, we can override the default values for the following: \n",
    "- The S3 bucket and prefix that you want to use for training and model data. This should be within the same region as the Notebook Instance, training, and hosting.\n",
    "- The IAM role arn used to give training and hosting access to your data. See the documentation for how to create these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_bucket = '<put your S3 bucket name>'  # replace with an existing bucket if needed\n",
    "\n",
    "s3_prefix = 'deepar-household-electricity-notebook'    # prefix used for all data stored within the bucket\n",
    "\n",
    "role = sagemaker.get_execution_role()             # IAM role to use by SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = sagemaker_session.boto_region_name\n",
    "\n",
    "s3_data_path = \"s3://{}/{}/data\".format(s3_bucket, s3_prefix)\n",
    "s3_output_path = \"s3://{}/{}/output\".format(s3_bucket, s3_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we configure the container image to be used for the region that we are running in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_name = sagemaker.amazon.amazon_estimator.get_image_uri(region, \"forecasting-deepar\", \"latest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing household electricity consumption dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After downloading the original dataset from the UCI ML repository, we load and parse the dataset. In addition, we modify dataset into a time-series formation supported by Pandas, so that we can utilize many features to handle time-series dataset (e.g. datetime, resampling, aggregation, basic statistics, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://www.allaboutcircuits.com/textbook/alternating-current/chpt-11/true-reactive-and-apparent-power/\n",
    "* https://circuitglobe.com/what-is-power-triangle.html\n",
    "* https://en.wikipedia.org/wiki/AC_power\n",
    "\n",
    "<img src=\"./power-triangle-compressor.jpg\" width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### White noise\n",
    "In discrete time, white noise is a discrete signal whose samples are regarded as a sequence of serially uncorrelated random variables with zero mean and finite variance. Depending on the context, one may also require that the samples be independent and having identical probability distribution (a.k.a. _i.i.d_). In particular, if each sample has a normal distribution with zero mean, the signal is said to be **Gaussian white noise**. \n",
    "\n",
    "Some properties of white noise:\n",
    "* White noise is the simplest example of a **stationary process**. <br>\n",
    "* if the lag is 0, auto-covariance will be a variance of probability distribution. Otherwise, auto-covariance will be 0. That is:\n",
    "<br><br>\n",
    "\\begin{equation}\n",
    "   \\gamma_l = \n",
    "    \\begin{cases}\n",
    "        Var[e_t] & \\text{for $l = 0$} \\\\\n",
    "        0 & \\text{for $l \\neq 0$} \n",
    "    \\end{cases}\n",
    "\\end{equation}\n",
    "<br>\n",
    "* if the lag is 0, auto-correlation will be 1. Otherwise, auto-correlation will be 0. That is:\n",
    "<br><br>\n",
    "\n",
    "\\begin{equation}\n",
    "   \\rho_l = \n",
    "    \\begin{cases}\n",
    "        1 & \\text{for $l = 0$} \\\\\n",
    "        0 & \\text{for $l \\neq 0$} \n",
    "    \\end{cases}\n",
    "\\end{equation}\n",
    "<br>\n",
    "* Gaussian white noise can be expressed by:\n",
    "<br><br>\n",
    "\\begin{equation}\n",
    "e_t \\sim \\text{ $i.i.d$ } N(\\mu,\\sigma^2) \\text{ for all $t$}\n",
    "\\end{equation}\n",
    "\n",
    "**Prewhitening:**\n",
    "A technique to process a time series data to make it behave statistically like white noise. (The 'pre' means that whitening precedes some other analytical approaches enabling to work better if the additive noise is white).\n",
    "\n",
    "https://datascienceschool.net/view-notebook/6b963e771dc54f8c8cb23437274a86d6/ <br>\n",
    "http://hosting.astro.cornell.edu/~cordes/A6523/Prewhitening.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_NAME = './household_power_consumption.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Note that dataset includes `nan` and `?` as a `string`. They need to be converted to numpy `nan` in importing stage and treated both of them the same.\n",
    "* Two columns `Date` and `Time` can be merged into one column `Date_Time`.\n",
    "* The index of dataset need to be reset (with `Date_Time`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Data can be downloaded from: http://archive.ics.uci.edu/ml/machine-learning-databases/00235/\n",
    "* Just open the zip file and grab the file 'household_power_consumption.txt' put it in the directory that you would like to run the code.\n",
    "* `infer_datetime_format`: to allow speedups for homogeneously formatted datetimes. `pd.read_csv` and `pd.to_datetime` learned a new `infer_datetime_format` keyword which greatly improves parsing perf in many cases. (http://pandas.pydata.org/pandas-docs/version/0.17.1/whatsnew.html#id55)\n",
    "* `low_memory`: Please refer the following link: https://stackoverflow.com/questions/24251219/pandas-read-csv-low-memory-and-dtype-options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://archive.ics.uci.edu/ml/machine-learning-databases/00235/household_power_consumption.zip\n",
    "!unzip household_power_consumption.zip\n",
    "!mv household_power_consumption.txt household_power_consumption.csv\n",
    "\n",
    "data = pd.read_csv(FILE_NAME, sep=\",\", parse_dates={'Date_Time': ['Date', 'Time']}, \n",
    "                  infer_datetime_format=True, na_values=['nan','?'], \n",
    "                  low_memory=False, index_col='Date_Time')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to check the type of data for each column. If some of them has `object` type, they should be converted into the numerical format (e.g. `float64`, `int64`).\n",
    "For example, we can use the following codes for the above tasks: <br>\n",
    "```\n",
    "data = data.convert_objects(convert_numeric=True)\n",
    "data.info()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we want to build an ML model, we need to understand dataset in the following perspectives:\n",
    "* The meaning of data for each feature (column)\n",
    "* Summarized information from the basic statistics\n",
    "* Relaionship or Association between features\n",
    "* Features having similar patterns\n",
    "* Trend or Periodicity\n",
    "* Outliers, Noisy data, Missing values\n",
    "* Data type (categorical data, numerical data, etc.)\n",
    "... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with missing values 'nan' with a test statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## finding all columns that have nan:\n",
    "\n",
    "droping_list_all=[]\n",
    "for j in range(0,7):\n",
    "    if not data.iloc[:, j].notnull().all():\n",
    "        droping_list_all.append(j)        \n",
    "        #print(df.iloc[:,j].unique())\n",
    "# droping_list_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filling nan with mean in any columns\n",
    "\n",
    "for j in range(0,7):        \n",
    "        data.iloc[:,j]=data.iloc[:,j].fillna(data.iloc[:,j].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another sanity check to make sure that there are not more any nan\n",
    "data.isnull().sum()\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make a transformed dataset with different frequency by using `resample()`. <br>\n",
    "**down-sampling:** <br>\n",
    "* To transform the original dataset to a lower frequencey\n",
    "* Summarize or aggregate the higher frequency data points (i.e. original dataset)\n",
    "* Example: 1 minute-based timestamps → 5 minute-based timestamps \n",
    "\n",
    "**up-sampling:** <br>\n",
    "* To transform the original dataset to a higher frequencey\n",
    "* the lower frequency data points (i.e. original dataset)\n",
    "* Example: 1 minute-based timestamps → 0.5 minute-based (i.e. 30 second-based) timestamps \n",
    "\n",
    "https://machinelearningmastery.com/resample-interpolate-time-series-data-python/ <br>\n",
    "**Resampling** <br>\n",
    "Resampling involves changing the frequency of your time series observations.\n",
    "\n",
    "Two types of resampling are:\n",
    "\n",
    "1. Upsampling: Where you increase the frequency of the samples, such as from minutes to seconds. For upsampling, `ffill()` (i.e. forward filling) or `bfill()` (i.e. backward filling) can be required to fill the newly created data points that was not available. \n",
    "1. Downsampling: Where you decrease the frequency of the samples, such as from days to months. For downsampling, some kind of aggregation operation can be needed. (e.g. `mean()`, `first()`, etc.)\n",
    "\n",
    "In both cases, data must be invented.\n",
    "\n",
    "In the case of upsampling, care may be needed in determining how the fine-grained observations are calculated using interpolation. In the case of downsampling, care may be needed in selecting the summary statistics used to calculate the new aggregated values.\n",
    "\n",
    "There are perhaps two main reasons why you may be interested in resampling your time series data:\n",
    "\n",
    "1. Problem Framing: Resampling may be required if your data is available at the same frequency that you want to make predictions.\n",
    "1. Feature Engineering: Resampling can also be used to provide additional structure or insight into the learning problem for supervised learning models.\n",
    "There is a lot of overlap between these two cases.\n",
    "\n",
    "For example, you may have daily data and want to predict a monthly problem. You could use the daily data directly or you could downsample it to monthly data and develop your model.\n",
    "\n",
    "A feature engineering perspective may use observations and summaries of observations from both time scales and more in developing a model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Global_active_power'].resample('M').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I resample over day, and show the sum and mean of Global_active_power. It is seen that mean and sum of resampled data set, have similar structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.Global_active_power.resample('D').sum().plot(title='Global_active_power resampled over day for sum') \n",
    "#df.Global_active_power.resample('D').mean().plot(title='Global_active_power resampled over day', color='red') \n",
    "plt.tight_layout()\n",
    "plt.show()   \n",
    "\n",
    "data.Global_active_power.resample('D').mean().plot(title='Global_active_power resampled over day for mean', color='red') \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I show mean and standard deviation (std) of 'Global_intensity' resampled over day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = data.Global_intensity.resample('D').agg(['mean', 'std'])\n",
    "r.plot(subplots = True, title='Global_intensity resampled over day')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I show mean and standard deviation (std) of 'Global_reactive_power' resampled over day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2 = data.Global_reactive_power.resample('D').agg(['mean', 'std'])\n",
    "r2.plot(subplots = True, title='Global_reactive_power resampled over day', color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I show sum of 'global_active_power' resampled over day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum of 'Global_active_power' resampled over month\n",
    "data['Global_active_power'].resample('M').mean().plot(kind='bar')\n",
    "plt.xticks(rotation=60)\n",
    "plt.ylabel('Global_active_power')\n",
    "plt.title('Global_active_power per month (averaged over month)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I show mean of 'global_active_power' resampled over day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Global_active_power'].resample('Q').mean().plot(kind='bar')\n",
    "plt.xticks(rotation=60)\n",
    "plt.ylabel('Global_active_power')\n",
    "plt.title('Global_active_power per quarter (averaged over quarter)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is very important to note from above two plots that resampling over larger time inteval, will diminish the periodicity of system as we expect. This is important for machine learning feature engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mean of 'Voltage' resampled over month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Voltage'].resample('M').mean().plot(kind='bar', color='red')\n",
    "plt.xticks(rotation=60)\n",
    "plt.ylabel('Voltage')\n",
    "plt.title('Voltage per quarter (summed over quarter)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Sub_metering_1'].resample('M').mean().plot(kind='bar', color='brown')\n",
    "plt.xticks(rotation=60)\n",
    "plt.ylabel('Sub_metering_1')\n",
    "plt.title('Sub_metering_1 per quarter (summed over quarter)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is seen from the above plots that the mean of 'Volage' over month is pretty much constant compared to other features. This is important again in feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of the mean of different features resampled over day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below I compare the mean of different featuresresampled over day. \n",
    "# specify columns to plot\n",
    "cols = [0, 1, 2, 3, 5, 6]\n",
    "i = 1\n",
    "values = data.resample('D').mean().values\n",
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot each column\n",
    "groups = list(data.columns.values)\n",
    "plt.figure(figsize=(15, 10))\n",
    "for group in cols:\n",
    "    plt.subplot(len(cols), 1, i)\n",
    "    plt.plot(values[:, group])\n",
    "    plt.title(data.columns[group], y=0.75, loc='right')\n",
    "    i += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## resampling over week and computing mean\n",
    "data.Global_reactive_power.resample('W').mean().plot(color='y', legend=True)\n",
    "data.Global_active_power.resample('W').mean().plot(color='r', legend=True)\n",
    "data.Sub_metering_1.resample('W').mean().plot(color='b', legend=True)\n",
    "data.Global_intensity.resample('W').mean().plot(color='g', legend=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below I show hist plot of the mean of different feature resampled over month \n",
    "data.Global_active_power.resample('M').mean().plot(kind='hist', alpha=0.3, legend=True )\n",
    "data.Global_reactive_power.resample('M').mean().plot(kind='hist', alpha=0.3, legend=True)\n",
    "#df.Voltage.resample('M').sum().plot(kind='hist',color='g', legend=True)\n",
    "data.Global_intensity.resample('M').mean().plot(kind='hist', alpha=0.3, legend=True)\n",
    "data.Sub_metering_1.resample('M').mean().plot(kind='hist', alpha=0.3, legend=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below I show hist plot of the mean of different feature resampled over month \n",
    "data.Global_active_power.resample('M').mean().plot(kind='hist', color='r', legend=True )\n",
    "#from pyqt_fit import kde\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of the mean of different features resampled over day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The correlations between 'Global_intensity', 'Global_active_power'\n",
    "# data_returns = data.pct_change()\n",
    "sns.jointplot(x='Global_intensity', y='Global_active_power', data=data)  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The correlations between 'Voltage' and  'Global_active_power'\n",
    "sns.jointplot(x='Voltage', y='Global_active_power', data=data)  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above two plots it is seen that 'Global_intensity' and 'Global_active_power' correlated. But 'Voltage', 'Global_active_power' are less correlated. This is important observation for machine learning purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison among features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlations among columns\n",
    "plt.matshow(data.corr(method='spearman'),vmax=1,vmin=-1,cmap='RdBu')\n",
    "plt.title('without resampling')\n",
    "plt.colorbar()\n",
    "plt.figure(figsize=(40,40))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of mean of resampled features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlations of mean of features resampled over months\n",
    "plt.matshow(data.resample('M').mean().corr(method='spearman'),vmax=1,vmin=-1,cmap='RdBu')\n",
    "plt.title('resampled over month', size=15)\n",
    "plt.colorbar()\n",
    "plt.margins(0.02)\n",
    "plt.matshow(data.resample('A').mean().corr(method='spearman'),vmax=1,vmin=-1,cmap='RdBu')\n",
    "plt.title('resampled over year', size=15)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is seen from above that with resampling techniques one can change the correlations among features. This is important for feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(data.corr())\n",
    "pd.plotting.scatter_matrix(data, figsize=(12, 12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please check your scatter_matrix as below: <br>\n",
    "<img src=\"./ScreenShot-2018-07-18-15-18-02.png\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### StatsModels\n",
    "\n",
    "`statsmodels` is a Python module that provides classes and functions for the estimation of many different statistical models, as well as for conducting statistical tests, and statistical data exploration. An extensive list of result statistics are available for each estimator. The results are tested against existing statistical packages to ensure that they are correct. The package is released under the open source Modified BSD (3-clause) license. The online documentation is hosted at [statsmodels.org](http://www.statsmodels.org/).\n",
    "\n",
    "1. Statistics `stats`\n",
    "    * statistical tests\n",
    "    * kernel density estimation\n",
    "    * generalized method of moments \n",
    "<br><br>\n",
    "1. Linear regression\n",
    "    * Linear model\n",
    "    * Generalized Linear Model (GLM)\n",
    "    * Robust Linear Model\n",
    "    * Linear Mixed Effects Model\n",
    "    * ANOVA (Analysis of Variance)\n",
    "    * Discrete Dependent Variable \n",
    "<br><br>\n",
    "1. Time-Series analysis\n",
    "    * ARMA/ARIMA process\n",
    "    * Vector ARMA process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries = data.Global_active_power.resample('D').mean()\n",
    "# for i in range(num_timeseries):\n",
    "#     timeseries.append(np.trim_zeros(data.iloc[:,i], trim='f'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(timeseries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Test splits\n",
    "\n",
    "Often times one is interested in evaluating the model or tuning its hyperparameters by looking at error metrics on a hold-out test set. Here we split the available data into train and test sets for evaluating the trained model. For standard machine learning tasks such as classification and regression, one typically obtains this split by randomly separating examples into train and test sets. However, in forecasting it is important to do this train/test split based on time rather than by time series.\n",
    "\n",
    "In this example, we will reserve the last section of each of the time series for evalutation purpose and use only the first part as training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use minute frequency for the time series\n",
    "freq = 'D'\n",
    "\n",
    "# we predict for 60 days \n",
    "prediction_length = 60\n",
    "\n",
    "# we also use 60 days as context length, \n",
    "# this is the number of state updates accomplished before making predictions\n",
    "context_length = 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We specify here the portion of the data that is used for training: the model sees data from 2006-12-16 to 2008-12-31 for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_dataset = pd.Timestamp(\"2006-12-16\", freq=freq)\n",
    "end_training = pd.Timestamp(\"2010-07-11\", freq=freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DeepAR JSON input format represents each time series as a JSON object. In the simplest case each time series just consists of a start time stamp (``start``) and a list of values (``target``). For more complex cases, DeepAR also supports the fields ``dynamic_feat`` for time-series features and ``cat`` for categorical features, which we will use  later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = [\n",
    "    {\n",
    "        \"start\": str(start_dataset),\n",
    "        \"target\": timeseries[start_dataset:end_training - 1].tolist()  # We use -1, because pandas indexing includes the upper bound \n",
    "        # \"target\": ts[start_dataset:end_training - 1].tolist()  # We use -1, because pandas indexing includes the upper bound \n",
    "    }\n",
    "    #for ts in timeseries\n",
    "]\n",
    "# print(len(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(timeseries[start_dataset:end_training -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = [\n",
    "    {\n",
    "        \"start\": str(start_dataset),\n",
    "        \"target\": timeseries[end_training:end_training + 10 * prediction_length].tolist()  # We use -1, because pandas indexing includes the upper bound \n",
    "    }\n",
    "    #for ts in timeseries\n",
    "]\n",
    "# print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(timeseries[end_training:end_training + 10 * prediction_length])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now write the dictionary to the `jsonlines` file format that DeepAR understands (it also supports gzipped jsonlines and parquet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dicts_to_file(path, data):\n",
    "    with open(path, 'wb') as fp:\n",
    "        for d in data:\n",
    "            fp.write(json.dumps(d).encode(\"utf-8\"))\n",
    "            fp.write(\"\\n\".encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "write_dicts_to_file(\"train.json\", training_data)\n",
    "write_dicts_to_file(\"test.json\", test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the data files locally, let us copy them to S3 where DeepAR can access them. Depending on your connection, this may take a couple of minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "def copy_to_s3(local_file, s3_path, override=False):\n",
    "    assert s3_path.startswith('s3://')\n",
    "    split = s3_path.split('/')\n",
    "    bucket = split[2]\n",
    "    path = '/'.join(split[3:])\n",
    "    buk = s3.Bucket(bucket)\n",
    "    \n",
    "    if len(list(buk.objects.filter(Prefix=path))) > 0:\n",
    "        if not override:\n",
    "            print('File s3://{}/{} already exists.\\nSet override to upload anyway.\\n'.format(s3_bucket, s3_path))\n",
    "            return\n",
    "        else:\n",
    "            print('Overwriting existing file')\n",
    "    with open(local_file, 'rb') as data:\n",
    "        print('Uploading file to {}'.format(s3_path))\n",
    "        buk.put_object(Key=path, Body=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "copy_to_s3(\"train.json\", s3_data_path + \"/train/train.json\")\n",
    "copy_to_s3(\"test.json\", s3_data_path + \"/test/test.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look to what we just wrote to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3filesystem = s3fs.S3FileSystem()\n",
    "with s3filesystem.open(s3_data_path + \"/train/train.json\", 'rb') as fp:\n",
    "    print(fp.readline().decode(\"utf-8\")[:100] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are all set with our dataset processing, we can now call DeepAR to train a model and generate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a model\n",
    "\n",
    "Here we define the estimator that will launch the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = sagemaker.estimator.Estimator(\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    image_name=image_name,\n",
    "    role=role,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.m4.xlarge',\n",
    "    base_job_name='deepar-home-electricity-demo',\n",
    "    output_path=s3_output_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to set the hyperparameters for the training job. For example frequency of the time series used, number of data points the model will look at in the past, number of predicted data points. The other hyperparameters concern the model to train (number of layers, number of cells per layer, likelihood function) and the training options (number of epochs, batch size, learning rate...). We use default parameters for every optional parameter in this case (you can always use [Sagemaker Automated Model Tuning](https://aws.amazon.com/blogs/aws/sagemaker-automatic-model-tuning/) to tune them)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters = {\n",
    "#     \"time_freq\": freq,\n",
    "#     \"epochs\": \"5\",\n",
    "#     \"early_stopping_patience\": \"10\",\n",
    "#     \"mini_batch_size\": \"20\",\n",
    "#     \"learning_rate\": \"0.001\",\n",
    "#     \"context_length\": str(context_length),\n",
    "#     \"prediction_length\": str(prediction_length)\n",
    "# }\n",
    "hyperparameters = {\n",
    "    \"time_freq\": freq,\n",
    "    \"context_length\": str(context_length),\n",
    "    \"prediction_length\": str(prediction_length),\n",
    "    \"num_cells\": \"40\",\n",
    "    \"num_layers\": \"3\",\n",
    "    \"likelihood\": \"gaussian\",\n",
    "    \"epochs\": \"20\",\n",
    "    \"mini_batch_size\": \"32\",\n",
    "    \"learning_rate\": \"0.001\",\n",
    "    \"dropout_rate\": \"0.05\",\n",
    "    \"early_stopping_patience\": \"10\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.set_hyperparameters(**hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to launch the training job. SageMaker will start an EC2 instance, download the data from S3, start training the model and save the trained model.\n",
    "\n",
    "If you provide the `test` data channel as we do in this example, DeepAR will also calculate accuracy metrics for the trained model on this test. This is done by predicting the last `prediction_length` points of each time-series in the test set and comparing this to the actual value of the time-series. \n",
    "\n",
    "**Note:** the next cell may take a few minutes to complete, depending on data size, model complexity, training options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "data_channels = {\n",
    "    \"train\": \"{}/train/\".format(s3_data_path),\n",
    "    \"test\": \"{}/test/\".format(s3_data_path)\n",
    "}\n",
    "\n",
    "estimator.fit(inputs=data_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since you pass a test set in this example, accuracy metrics for the forecast are computed and logged (see bottom of the log).\n",
    "You can find the definition of these metrics from [our documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/deepar.html). You can use these to optimize the parameters and tune your model or use SageMaker's [Automated Model Tuning service](https://aws.amazon.com/blogs/aws/sagemaker-automatic-model-tuning/) to tune the model for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create endpoint and predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a trained model, we can use it to perform predictions by deploying it to an endpoint.\n",
    "\n",
    "**Note: Remember to delete the endpoint after running this experiment. A cell at the very bottom of this notebook will do that: make sure you run it at the end.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To query the endpoint and perform predictions, we can define the following utility class: this allows making requests using `pandas.Series` objects rather than raw JSON strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepARPredictor(sagemaker.predictor.RealTimePredictor):\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, content_type=sagemaker.content_types.CONTENT_TYPE_JSON, **kwargs)\n",
    "        \n",
    "    def predict(self, ts, cat=None, dynamic_feat=None, \n",
    "                num_samples=100, return_samples=False, quantiles=[\"0.1\", \"0.5\", \"0.9\"]):\n",
    "        \"\"\"Requests the prediction of for the time series listed in `ts`, each with the (optional)\n",
    "        corresponding category listed in `cat`.\n",
    "        \n",
    "        ts -- `pandas.Series` object, the time series to predict\n",
    "        cat -- integer, the group associated to the time series (default: None)\n",
    "        num_samples -- integer, number of samples to compute at prediction time (default: 100)\n",
    "        return_samples -- boolean indicating whether to include samples in the response (default: False)\n",
    "        quantiles -- list of strings specifying the quantiles to compute (default: [\"0.1\", \"0.5\", \"0.9\"])\n",
    "        \n",
    "        Return value: list of `pandas.DataFrame` objects, each containing the predictions\n",
    "        \"\"\"\n",
    "        prediction_time = ts.index[-1] + 1\n",
    "        quantiles = [str(q) for q in quantiles]\n",
    "        req = self.__encode_request(ts, cat, dynamic_feat, num_samples, return_samples, quantiles)\n",
    "        res = super(DeepARPredictor, self).predict(req)\n",
    "        return self.__decode_response(res, ts.index.freq, prediction_time, return_samples)\n",
    "    \n",
    "    def __encode_request(self, ts, cat, dynamic_feat, num_samples, return_samples, quantiles):\n",
    "        instance = series_to_dict(ts, cat if cat is not None else None, dynamic_feat if dynamic_feat else None)\n",
    "\n",
    "        configuration = {\n",
    "            \"num_samples\": num_samples,\n",
    "            \"output_types\": [\"quantiles\", \"samples\"] if return_samples else [\"quantiles\"],\n",
    "            \"quantiles\": quantiles\n",
    "        }\n",
    "        \n",
    "        http_request_data = {\n",
    "            \"instances\": [instance],\n",
    "            \"configuration\": configuration\n",
    "        }\n",
    "        \n",
    "        return json.dumps(http_request_data).encode('utf-8')\n",
    "    \n",
    "    def __decode_response(self, response, freq, prediction_time, return_samples):\n",
    "        # we only sent one time series so we only receive one in return\n",
    "        # however, if possible one will pass multiple time series as predictions will then be faster\n",
    "        predictions = json.loads(response.decode('utf-8'))['predictions'][0]\n",
    "        prediction_length = len(next(iter(predictions['quantiles'].values())))\n",
    "        prediction_index = pd.DatetimeIndex(start=prediction_time, freq=freq, periods=prediction_length)        \n",
    "        if return_samples:\n",
    "            dict_of_samples = {'sample_' + str(i): s for i, s in enumerate(predictions['samples'])}\n",
    "        else:\n",
    "            dict_of_samples = {}\n",
    "        return pd.DataFrame(data={**predictions['quantiles'], **dict_of_samples}, index=prediction_index)\n",
    "\n",
    "    def set_frequency(self, freq):\n",
    "        self.freq = freq\n",
    "        \n",
    "def encode_target(ts):\n",
    "    return [x if np.isfinite(x) else \"NaN\" for x in ts]        \n",
    "\n",
    "def series_to_dict(ts, cat=None, dynamic_feat=None):\n",
    "    \"\"\"Given a pandas.Series object, returns a dictionary encoding the time series.\n",
    "\n",
    "    ts -- a pands.Series object with the target time series\n",
    "    cat -- an integer indicating the time series category\n",
    "\n",
    "    Return value: a dictionary\n",
    "    \"\"\"\n",
    "    obj = {\"start\": str(ts.index[0]), \"target\": encode_target(ts)}\n",
    "    if cat is not None:\n",
    "        obj[\"cat\"] = cat\n",
    "    if dynamic_feat is not None:\n",
    "        obj[\"dynamic_feat\"] = dynamic_feat        \n",
    "    return obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can deploy the model and create and endpoint that can be queried using our custom DeepARPredictor class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = estimator.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m4.xlarge',\n",
    "    predictor_cls=DeepARPredictor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions and plot results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the `predictor` object to generate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.predict(timeseries[:-prediction_length], quantiles=[0.10, 0.5, 0.90])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we define a plotting function that queries the model and displays the forecast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(\n",
    "    predictor, \n",
    "    target_ts, \n",
    "    cat=None, \n",
    "    dynamic_feat=None, \n",
    "    forecast_date=end_training, \n",
    "    show_samples=False, \n",
    "    plot_history=7 * 12,\n",
    "    confidence=80\n",
    "):\n",
    "    print(\"calling served model to generate predictions starting from {}\".format(str(forecast_date)))\n",
    "    assert(confidence > 50 and confidence < 100)\n",
    "    low_quantile = 0.5 - confidence * 0.005\n",
    "    up_quantile = confidence * 0.005 + 0.5\n",
    "        \n",
    "    # we first construct the argument to call our model\n",
    "    args = {\n",
    "        \"ts\": target_ts[:forecast_date],\n",
    "        \"return_samples\": show_samples,\n",
    "        \"quantiles\": [low_quantile, 0.5, up_quantile],\n",
    "        \"num_samples\": 100\n",
    "    }\n",
    "\n",
    "\n",
    "    if dynamic_feat is not None:\n",
    "        args[\"dynamic_feat\"] = dynamic_feat\n",
    "        fig = plt.figure(figsize=(20, 6))\n",
    "        ax = plt.subplot(2, 1, 1)\n",
    "    else:\n",
    "        fig = plt.figure(figsize=(20, 3))\n",
    "        ax = plt.subplot(1,1,1)\n",
    "    \n",
    "    if cat is not None:\n",
    "        args[\"cat\"] = cat\n",
    "        ax.text(0.9, 0.9, 'cat = {}'.format(cat), transform=ax.transAxes)\n",
    "\n",
    "    # call the end point to get the prediction\n",
    "    prediction = predictor.predict(**args)\n",
    "\n",
    "    # plot the samples\n",
    "    if show_samples: \n",
    "        for key in prediction.keys():\n",
    "            if \"sample\" in key:\n",
    "                prediction[key].plot(color='lightskyblue', alpha=0.2, label='_nolegend_')\n",
    "                \n",
    "                \n",
    "    # plot the target\n",
    "    target_section = target_ts[forecast_date-plot_history:forecast_date+prediction_length]\n",
    "    target_section.plot(color=\"black\", label='target')\n",
    "    \n",
    "    # plot the confidence interval and the median predicted\n",
    "    ax.fill_between(\n",
    "        prediction[str(low_quantile)].index, \n",
    "        prediction[str(low_quantile)].values, \n",
    "        prediction[str(up_quantile)].values, \n",
    "        color=\"b\", alpha=0.3, label='{}% confidence interval'.format(confidence)\n",
    "    )\n",
    "    prediction[\"0.5\"].plot(color=\"b\", label='P50')\n",
    "    ax.legend(loc=2)    \n",
    "    \n",
    "    # fix the scale as the samples may change it\n",
    "    ax.set_ylim(target_section.min() * 0.5, target_section.max() * 1.5)\n",
    "    \n",
    "    if dynamic_feat is not None:\n",
    "        for i, f in enumerate(dynamic_feat, start=1):\n",
    "            ax = plt.subplot(len(dynamic_feat) * 2, 1, len(dynamic_feat) + i, sharex=ax)\n",
    "            feat_ts = pd.Series(\n",
    "                index=pd.DatetimeIndex(start=target_ts.index[0], freq=target_ts.index.freq, periods=len(f)),\n",
    "                data=f\n",
    "            )\n",
    "            feat_ts[forecast_date-plot_history:forecast_date+prediction_length].plot(ax=ax, color='g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can interact with the function previously defined, to look at the forecast of any customer at any point in (future) time. \n",
    "\n",
    "For each request, the predictions are obtained by calling our served model on the fly.\n",
    "\n",
    "Here we forecast the consumption of an office after week-end (note the lower week-end consumption). \n",
    "You can select any time series and any forecast date, just click on `Run Interact` to generate the predictions from our served endpoint and see the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style = {'description_width': 'initial'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_df = predictor.predict(timeseries[:-prediction_length])\n",
    "actual_data = timeseries[-prediction_length:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(actual_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,6))\n",
    "timeseries[800:-len(list_of_df)].plot(color='#C3C8C4', linewidth=1.0)\n",
    "p10 = list_of_df['0.1']\n",
    "p90 = list_of_df['0.9']\n",
    "plt.fill_between(p10.index, p10, p90, color='#C5F7AB', alpha=0.5, label='80% confidence interval')\n",
    "actual_data.plot(color='#FCE08F', label='target')\n",
    "list_of_df['0.5'].plot(marker='^', linewidth=3.0, label='prediction median')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Additional features\n",
    "\n",
    "DeepAR has additional features such as handling the missing values as below:\n",
    "\n",
    "* missing values: DeepAR can handle missing values in the time series during training as well as for inference.\n",
    "* Additional time features: DeepAR provides a set default time series features such as hour of day etc. However, you can provide additional feature time series via the `dynamic_feat` field. \n",
    "* generalize frequencies: any integer multiple of the previously supported base frequencies (minutes `min`, hours `H`, days `D`, weeks `W`, month `M`) are now allowed; e.g., `15min`. We already demonstrated this above by using `2H` frequency.\n",
    "* categories: If your time series belong to different groups (e.g. types of product, regions, etc), this information can be encoded as one or more categorical features using the `cat` field.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the previous results, we will be able to impelement the advanced models to support the above features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
